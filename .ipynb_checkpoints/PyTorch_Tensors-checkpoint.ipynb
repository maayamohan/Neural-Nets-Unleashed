{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to PyTorch Tensors\n",
        "\n",
        "- Tensors are the basic units of every PyTorch program.\n",
        "- Tensors are really similar to the concept of Numpy arrays, the only and most significant difference being: They can be run on the *GPU*."
      ],
      "metadata": {
        "id": "dXK1NihkqaSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First we import PyTorch!"
      ],
      "metadata": {
        "id": "6aC7aeX_tg9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"Your PyTorch is on version:{torch.__version__}.\")"
      ],
      "metadata": {
        "id": "VSCffJSytfsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dear god, give me a tensor."
      ],
      "metadata": {
        "id": "iT94EhWttCnV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAAWLsA9qQt1"
      },
      "outputs": [],
      "source": [
        "myNewTens = torch.empty(2,3)\n",
        "print(myNewTens.shape)\n",
        "print(myNewTens.dtype)\n",
        "print(myNewTens.device)\n",
        "print(myNewTens.requires_grad)\n",
        "print(myNewTens.grad)\n",
        "print(myNewTens) # will contain random garbage since we've not initialized it"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Yay! We just created our *first* tensor!\n",
        "\n",
        "Now lets take a breather and think of what we just did.\n",
        "- You just created a $2 \\times 3$ tensor.\n",
        "- `shape` tells us the dimensions of the tensor.\n",
        "- `dtype` tells us the *type* of data stored within this tensor.\n",
        "- `device` tells us the device on which the tensor is allocated.\n",
        "- `requires_grad` determines whether gradients must be computed for the tensor.\n",
        "- `grad` is None on startup but becomes a tensor of gradients after a backward pass."
      ],
      "metadata": {
        "id": "aH2k5QvQuPXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Playing around with Tensors"
      ],
      "metadata": {
        "id": "cuWGvE43hQ6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Tensors from Python lists\n",
        "l = [1, 2, 3]\n",
        "tensor = torch.Tensor(l)\n",
        "print(tensor)\n",
        "# You could even stack multiple lists togeather and make a multidimensional tensor\n",
        "l1 = [1, 2, 3]\n",
        "l2 = [4, 5, 6]\n",
        "tensor = torch.Tensor([l1, l2])\n",
        "print(tensor)"
      ],
      "metadata": {
        "id": "62nuBohjhyAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can create a Tensor filled with random numbers\n",
        "tensor = torch.rand(2, 3)\n",
        "print(tensor)"
      ],
      "metadata": {
        "id": "th0GBfOYh90K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a matrix of all zerosedit\n",
        "tensor = torch.zeros(2, 3)\n",
        "print(tensor)"
      ],
      "metadata": {
        "id": "keDVW8lqie55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a matrix of all zeros and explicitly set data type to be double\n",
        "tensor = torch.zeros(2, 3, dtype=torch.double)\n",
        "print(tensor)"
      ],
      "metadata": {
        "id": "U3ubnkNqioCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Common Tensor operations"
      ],
      "metadata": {
        "id": "LrPDCVwejKF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding size of a 1-D tensor\n",
        "tensor = torch.zeros(2, 3)\n",
        "print(tensor.size())\n",
        "\n",
        "# Finding the size of a 2-D tensor\n",
        "tensor = torch.zeros(2, 3, 4)\n",
        "print(tensor.size())\n",
        "\n",
        "# Finding the size of a 3-D tensor\n",
        "tensor = torch.zeros(2, 3, 4, 5)\n",
        "print(tensor.size())\n"
      ],
      "metadata": {
        "id": "9_3v1wteisQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Arithmetic operations on 2 tensors\n",
        "x = torch.rand(2, 3)\n",
        "y = torch.rand(2, 3)\n",
        "z = x + y\n",
        "\n",
        "print(f\"x:\\n{x}\\n\")\n",
        "print(f\"y:\\n{y}\\n\")\n",
        "print(f\"z = x + y:\\n{z}\\n\")"
      ],
      "metadata": {
        "id": "1HTu3MJHiy8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Special \"inplace\" functions\n",
        "\n",
        "y.add_(x)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "tkYL4c1lkPhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Methods (usually methods ending with an underscore like `add_()`) are called **In-place** operations.\n",
        "This means that they don't make a *copy* of the result in memory. They literally perform the operation on the `y` matrix. This is crucial for memory sensitive aplications.\n",
        "\n",
        "Here's a *great* blog on the [Dangers of Inplace Methods](https://lernapparat.de/pytorch-inplace)"
      ],
      "metadata": {
        "id": "AcbNhvuokiqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Indexing into a Tensor\n",
        "x = torch.rand(2, 3)\n",
        "print(x)\n",
        "print(x[1, 1])"
      ],
      "metadata": {
        "id": "oR6G2DQekh2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Broadcasting tensors\n",
        "x = torch.rand(2, 3)\n",
        "y = torch.rand(3)\n",
        "print(x)\n",
        "print(y)\n",
        "z = x + y\n",
        "print(z)"
      ],
      "metadata": {
        "id": "jhFO7PmNJIwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshaping tensors\n",
        "x = torch.rand(2, 3)\n",
        "print(x)\n",
        "y = x.view(3, 2)\n",
        "print(y)\n",
        "z = x.view(6)\n",
        "print(z)\n",
        "w = x.view(-1, 2) # -1 is inferred from other dimeensions\n",
        "print(w)"
      ],
      "metadata": {
        "id": "MON4jdLoond_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create a PyTorch tensor\n",
        "tensor = torch.ones(5)\n",
        "print(f\"PyTorch Tensor: {tensor}\")\n",
        "\n",
        "# Convert the PyTorch tensor to a NumPy array\n",
        "numpy_array = tensor.numpy()\n",
        "print(f\"NumPy Array: {numpy_array}\")\n",
        "\n",
        "# Convert a NumPy array to a PyTorch tensor\n",
        "numpy_array = np.array([1, 2, 3])\n",
        "tensor = torch.from_numpy(numpy_array)\n",
        "print(f\"Tensor from NumPy Array: {tensor}\")\n"
      ],
      "metadata": {
        "id": "nAOIKJuPpKCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Optional] Moving PyTorch Tensors onto the GPU ⚡️\n",
        "(Applicable only if your system has a CUDA enabled GPU)\n",
        "\n"
      ],
      "metadata": {
        "id": "6xD2puc5qQZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")          # a CUDA device object\n",
        "  x = torch.ones(5, device=device)       # directly create a tensor on GPU\n",
        "  y = torch.ones(5)\n",
        "  y = y.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
        "  z = x + y\n",
        "  print(z)\n",
        "  print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!\n",
        "else:\n",
        "  print(\"CUDA is not available.\")\n"
      ],
      "metadata": {
        "id": "MQnXaNp0qwDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autograd\n",
        "\n",
        "- PyTorch's Autodiff tool.\n",
        "- Tracks the operations performed on a tensors in a forward pass.\n",
        "- When `backward()` is called gradients are auto-computed."
      ],
      "metadata": {
        "id": "PN1LptnZryTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz"
      ],
      "metadata": {
        "collapsed": true,
        "id": "B3c_CtFFNu2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor with autograd enabled\n",
        "x = torch.tensor(torch.rand(2,3), requires_grad=True)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "oOdNQB1yq4dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform some operation on the tensor and print it\n",
        "y = x * 2\n",
        "print(y)"
      ],
      "metadata": {
        "id": "us5HAOcssUFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform some more combinations of operations\n",
        "z = y.mean()\n",
        "print(z)"
      ],
      "metadata": {
        "id": "0s88yt0qsYLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets see what the computation graph looks like\n",
        "import torchviz\n",
        "torchviz.make_dot(z, params={'x': x})"
      ],
      "metadata": {
        "id": "b-LMVruPPUqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform backpropagation\n",
        "z.backward()\n",
        "\n",
        "# Print the gradients of x\n",
        "print(x.grad)\n",
        "\n",
        "# Disable gradient tracking\n",
        "with torch.no_grad():\n",
        "  x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "  y = x * 2\n",
        "  print(y.requires_grad)\n",
        "\n",
        "\n",
        "# Another way to disable gradient tracking\n",
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "y = x.detach() * 2\n",
        "print(y.requires_grad)\n",
        "\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x.requires_grad)\n",
        "y = x.detach()\n",
        "print(y.requires_grad)\n",
        "x.requires_grad_(False)\n",
        "print(x.requires_grad)"
      ],
      "metadata": {
        "id": "lDS4f4Q2stvm",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}