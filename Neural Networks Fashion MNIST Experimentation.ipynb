{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50eb7b9e-188a-4371-bc09-ddf96769c36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 14:04:38.094681: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# importing required modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8d899a3-f423-449a-884e-b6f03dd4353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    # Legacy Python that doesn't verify HTTPS certificates by default\n",
    "    pass\n",
    "else:\n",
    "    # Handle target environment that doesn't support HTTPS verification\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a4b2b7e-3b41-44b4-9235-00038f2e4e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3169e898-c1bb-4457-b04c-a3f9e9221dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if all data is loaded properly\n",
    "assert x_train.shape == (60000, 28, 28)\n",
    "assert x_test.shape == (10000, 28, 28)\n",
    "assert y_train.shape == (60000,)\n",
    "assert y_test.shape == (10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb9c16bf-aba3-439a-a4c1-09abbe707baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattening and normalizing the input data\n",
    "x_train = x_train.reshape(x_train.shape[0], -1) / 255.0\n",
    "x_test = x_test.reshape(x_test.shape[0], -1) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89228d39-00e9-4924-b59b-6be3d5706854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding (creating an identity matrix corresponding to the number of categories)\n",
    "def one_hot_encode(y, num_classes):\n",
    "    return np.eye(num_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da0a91b9-2281-4a03-aa3e-1c4bd357cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying one-hot encoding (creating 10x10 matrix for the 10 categories of digits)\n",
    "y_train_encoded = one_hot_encode(y_train, 10)\n",
    "y_test_encoded = one_hot_encode(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7a0d62f-e1ca-4d60-982b-a591c2d3f310",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01, l1_lambda=0.0, l2_lambda=0.0):\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l1_lambda = l1_lambda\n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = np.tanh(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.softmax(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        m = X.shape[0]\n",
    "        output_error = output - y\n",
    "        hidden_error = np.dot(output_error, self.W2.T) * (1 - np.tanh(self.z1) ** 2)\n",
    "\n",
    "        dW2 = np.dot(self.a1.T, output_error) / m\n",
    "        db2 = np.sum(output_error, axis=0, keepdims=True) / m\n",
    "        dW1 = np.dot(X.T, hidden_error) / m\n",
    "        db1 = np.sum(hidden_error, axis=0, keepdims=True) / m\n",
    "\n",
    "        # L1 regularization\n",
    "        dW1 += self.l1_lambda * np.sign(self.W1)\n",
    "        dW2 += self.l1_lambda * np.sign(self.W2)\n",
    "\n",
    "        # L2 regularization\n",
    "        dW1 += self.l2_lambda * self.W1\n",
    "        dW2 += self.l2_lambda * self.W2\n",
    "\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "\n",
    "    def train(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, output)\n",
    "            if epoch % 10 == 0:\n",
    "                loss = -np.mean(np.sum(y * np.log(output + 1e-10), axis=1))\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.forward(X)\n",
    "        return np.argmax(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f61728e5-1d3e-4a7d-a0d2-7a067a7c33ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 784  # 28x28 pixels\n",
    "hidden_size = 64  # Number of neurons in the hidden layer\n",
    "output_size = 10   # 10 classes for digits 0-9\n",
    "learning_rate = 0.5\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9b3fb36-58cc-4233-9436-f77b74e17f24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.3036\n",
      "Epoch 10, Loss: 1.4640\n",
      "Epoch 20, Loss: 1.1372\n",
      "Epoch 30, Loss: 0.9007\n",
      "Epoch 40, Loss: 0.8005\n",
      "Epoch 50, Loss: 0.7401\n",
      "Epoch 60, Loss: 0.7060\n",
      "Epoch 70, Loss: 0.6485\n",
      "Epoch 80, Loss: 0.6386\n",
      "Epoch 90, Loss: 0.6293\n",
      "Epoch 100, Loss: 0.6367\n",
      "Epoch 110, Loss: 0.5739\n",
      "Epoch 120, Loss: 0.5715\n",
      "Epoch 130, Loss: 0.5689\n",
      "Epoch 140, Loss: 0.5551\n",
      "Epoch 150, Loss: 0.5363\n",
      "Epoch 160, Loss: 0.5702\n",
      "Epoch 170, Loss: 0.5130\n",
      "Epoch 180, Loss: 0.5215\n",
      "Epoch 190, Loss: 0.4941\n",
      "Epoch 200, Loss: 0.5072\n",
      "Epoch 210, Loss: 0.5207\n",
      "Epoch 220, Loss: 0.5057\n",
      "Epoch 230, Loss: 0.5341\n",
      "Epoch 240, Loss: 0.4628\n",
      "Epoch 250, Loss: 0.5164\n",
      "Epoch 260, Loss: 0.5035\n",
      "Epoch 270, Loss: 0.4952\n",
      "Epoch 280, Loss: 0.4874\n",
      "Epoch 290, Loss: 0.4507\n",
      "Epoch 300, Loss: 0.4703\n",
      "Epoch 310, Loss: 0.4519\n",
      "Epoch 320, Loss: 0.4511\n",
      "Epoch 330, Loss: 0.4561\n",
      "Epoch 340, Loss: 0.4330\n",
      "Epoch 350, Loss: 0.4435\n",
      "Epoch 360, Loss: 0.4331\n",
      "Epoch 370, Loss: 0.4300\n",
      "Epoch 380, Loss: 0.4269\n",
      "Epoch 390, Loss: 0.4354\n",
      "Epoch 400, Loss: 0.4240\n",
      "Epoch 410, Loss: 0.4367\n",
      "Epoch 420, Loss: 0.4180\n",
      "Epoch 430, Loss: 0.4139\n",
      "Epoch 440, Loss: 0.4250\n",
      "Epoch 450, Loss: 0.4177\n",
      "Epoch 460, Loss: 0.4155\n",
      "Epoch 470, Loss: 0.4089\n",
      "Epoch 480, Loss: 0.4114\n",
      "Epoch 490, Loss: 0.4381\n",
      "Epoch 500, Loss: 0.3966\n",
      "Epoch 510, Loss: 0.3960\n",
      "Epoch 520, Loss: 0.4066\n",
      "Epoch 530, Loss: 0.4084\n",
      "Epoch 540, Loss: 0.4117\n",
      "Epoch 550, Loss: 0.3937\n",
      "Epoch 560, Loss: 0.4049\n",
      "Epoch 570, Loss: 0.3885\n",
      "Epoch 580, Loss: 0.3880\n",
      "Epoch 590, Loss: 0.3941\n",
      "Epoch 600, Loss: 0.3812\n",
      "Epoch 610, Loss: 0.3997\n",
      "Epoch 620, Loss: 0.3835\n",
      "Epoch 630, Loss: 0.3849\n",
      "Epoch 640, Loss: 0.3814\n",
      "Epoch 650, Loss: 0.3823\n",
      "Epoch 660, Loss: 0.3791\n",
      "Epoch 670, Loss: 0.3861\n",
      "Epoch 680, Loss: 0.3729\n",
      "Epoch 690, Loss: 0.3768\n",
      "Epoch 700, Loss: 0.3795\n",
      "Epoch 710, Loss: 0.3683\n",
      "Epoch 720, Loss: 0.3851\n",
      "Epoch 730, Loss: 0.3718\n",
      "Epoch 740, Loss: 0.3687\n",
      "Epoch 750, Loss: 0.3649\n",
      "Epoch 760, Loss: 0.3777\n",
      "Epoch 770, Loss: 0.3723\n",
      "Epoch 780, Loss: 0.3686\n",
      "Epoch 790, Loss: 0.3795\n",
      "Epoch 800, Loss: 0.3619\n",
      "Epoch 810, Loss: 0.3874\n",
      "Epoch 820, Loss: 0.3568\n",
      "Epoch 830, Loss: 0.3727\n",
      "Epoch 840, Loss: 0.3617\n",
      "Epoch 850, Loss: 0.3565\n",
      "Epoch 860, Loss: 0.3615\n",
      "Epoch 870, Loss: 0.3655\n",
      "Epoch 880, Loss: 0.3543\n",
      "Epoch 890, Loss: 0.3613\n",
      "Epoch 900, Loss: 0.3601\n",
      "Epoch 910, Loss: 0.3632\n",
      "Epoch 920, Loss: 0.3567\n",
      "Epoch 930, Loss: 0.3589\n",
      "Epoch 940, Loss: 0.3500\n",
      "Epoch 950, Loss: 0.3576\n",
      "Epoch 960, Loss: 0.3448\n",
      "Epoch 970, Loss: 0.3485\n",
      "Epoch 980, Loss: 0.3505\n",
      "Epoch 990, Loss: 0.3516\n",
      "Test Accuracy (No Regularization): 0.8607\n"
     ]
    }
   ],
   "source": [
    "# Train without regularization\n",
    "nn_no_reg = NeuralNetwork(input_size, hidden_size, output_size, learning_rate)\n",
    "nn_no_reg.train(x_train, y_train_encoded, epochs)\n",
    "predictions_no_reg = nn_no_reg.predict(x_test)\n",
    "accuracy_no_reg = np.mean(predictions_no_reg == y_test)\n",
    "print(f\"Test Accuracy (No Regularization): {accuracy_no_reg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a1110c0-993e-4009-b9f2-69b20981f62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.3043\n",
      "Epoch 10, Loss: 1.4979\n",
      "Epoch 20, Loss: 1.3549\n",
      "Epoch 30, Loss: 0.9169\n",
      "Epoch 40, Loss: 0.8239\n",
      "Epoch 50, Loss: 0.7550\n",
      "Epoch 60, Loss: 0.7145\n",
      "Epoch 70, Loss: 0.6763\n",
      "Epoch 80, Loss: 0.6220\n",
      "Epoch 90, Loss: 0.6186\n",
      "Epoch 100, Loss: 0.6110\n",
      "Epoch 110, Loss: 0.6124\n",
      "Epoch 120, Loss: 0.5658\n",
      "Epoch 130, Loss: 0.5759\n",
      "Epoch 140, Loss: 0.5551\n",
      "Epoch 150, Loss: 0.6335\n",
      "Epoch 160, Loss: 0.5890\n",
      "Epoch 170, Loss: 0.5341\n",
      "Epoch 180, Loss: 0.5482\n",
      "Epoch 190, Loss: 0.5729\n",
      "Epoch 200, Loss: 0.5491\n",
      "Epoch 210, Loss: 0.5370\n",
      "Epoch 220, Loss: 0.5387\n",
      "Epoch 230, Loss: 0.5760\n",
      "Epoch 240, Loss: 0.5319\n",
      "Epoch 250, Loss: 0.5087\n",
      "Epoch 260, Loss: 0.5137\n",
      "Epoch 270, Loss: 0.5334\n",
      "Epoch 280, Loss: 0.4922\n",
      "Epoch 290, Loss: 0.4943\n",
      "Epoch 300, Loss: 0.4887\n",
      "Epoch 310, Loss: 0.5201\n",
      "Epoch 320, Loss: 0.5219\n",
      "Epoch 330, Loss: 0.5269\n",
      "Epoch 340, Loss: 0.4628\n",
      "Epoch 350, Loss: 0.4821\n",
      "Epoch 360, Loss: 0.4500\n",
      "Epoch 370, Loss: 0.4525\n",
      "Epoch 380, Loss: 0.4500\n",
      "Epoch 390, Loss: 0.4563\n",
      "Epoch 400, Loss: 0.4605\n",
      "Epoch 410, Loss: 0.4960\n",
      "Epoch 420, Loss: 0.4724\n",
      "Epoch 430, Loss: 0.5015\n",
      "Epoch 440, Loss: 0.4512\n",
      "Epoch 450, Loss: 0.4818\n",
      "Epoch 460, Loss: 0.4398\n",
      "Epoch 470, Loss: 0.4620\n",
      "Epoch 480, Loss: 0.4933\n",
      "Epoch 490, Loss: 0.4490\n",
      "Epoch 500, Loss: 0.4464\n",
      "Epoch 510, Loss: 0.4378\n",
      "Epoch 520, Loss: 0.5214\n",
      "Epoch 530, Loss: 0.4575\n",
      "Epoch 540, Loss: 0.4473\n",
      "Epoch 550, Loss: 0.4635\n",
      "Epoch 560, Loss: 0.4276\n",
      "Epoch 570, Loss: 0.4462\n",
      "Epoch 580, Loss: 0.4348\n",
      "Epoch 590, Loss: 0.4225\n",
      "Epoch 600, Loss: 0.4277\n",
      "Epoch 610, Loss: 0.4492\n",
      "Epoch 620, Loss: 0.4544\n",
      "Epoch 630, Loss: 0.4280\n",
      "Epoch 640, Loss: 0.4486\n",
      "Epoch 650, Loss: 0.4465\n",
      "Epoch 660, Loss: 0.4395\n",
      "Epoch 670, Loss: 0.4522\n",
      "Epoch 680, Loss: 0.4089\n",
      "Epoch 690, Loss: 0.4688\n",
      "Epoch 700, Loss: 0.4735\n",
      "Epoch 710, Loss: 0.4458\n",
      "Epoch 720, Loss: 0.4210\n",
      "Epoch 730, Loss: 0.4117\n",
      "Epoch 740, Loss: 0.4754\n",
      "Epoch 750, Loss: 0.3963\n",
      "Epoch 760, Loss: 0.4282\n",
      "Epoch 770, Loss: 0.4377\n",
      "Epoch 780, Loss: 0.3999\n",
      "Epoch 790, Loss: 0.4355\n",
      "Epoch 800, Loss: 0.4123\n",
      "Epoch 810, Loss: 0.3937\n",
      "Epoch 820, Loss: 0.4053\n",
      "Epoch 830, Loss: 0.4246\n",
      "Epoch 840, Loss: 0.4357\n",
      "Epoch 850, Loss: 0.4209\n",
      "Epoch 860, Loss: 0.4066\n",
      "Epoch 870, Loss: 0.3995\n",
      "Epoch 880, Loss: 0.3913\n",
      "Epoch 890, Loss: 0.4129\n",
      "Epoch 900, Loss: 0.4011\n",
      "Epoch 910, Loss: 0.4225\n",
      "Epoch 920, Loss: 0.3853\n",
      "Epoch 930, Loss: 0.4056\n",
      "Epoch 940, Loss: 0.4164\n",
      "Epoch 950, Loss: 0.4595\n",
      "Epoch 960, Loss: 0.3870\n",
      "Epoch 970, Loss: 0.4083\n",
      "Epoch 980, Loss: 0.3879\n",
      "Epoch 990, Loss: 0.4295\n",
      "Test Accuracy (L1 Regularization): 0.8371\n"
     ]
    }
   ],
   "source": [
    "# Train with L1 regularization\n",
    "nn_l1 = NeuralNetwork(input_size, hidden_size, output_size, learning_rate, l1_lambda=0.0001)\n",
    "nn_l1.train(x_train, y_train_encoded, epochs)\n",
    "predictions_l1 = nn_l1.predict(x_test)\n",
    "accuracy_l1 = np.mean(predictions_l1 == y_test)\n",
    "print(f\"Test Accuracy (L1 Regularization): {accuracy_l1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd4309ce-5a8e-4edb-9af5-a7aa62245eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 784  # 28x28 pixels\n",
    "hidden_size = 64  # Number of neurons in the hidden layer\n",
    "output_size = 10   # 10 classes for digits 0-9\n",
    "learning_rate = 0.5\n",
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb89ff3-acbc-48e1-9680-a3be22140dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.3030\n",
      "Epoch 10, Loss: 1.4539\n",
      "Epoch 20, Loss: 1.0136\n",
      "Epoch 30, Loss: 0.9170\n",
      "Epoch 40, Loss: 1.0347\n",
      "Epoch 50, Loss: 0.7169\n",
      "Epoch 60, Loss: 0.6892\n",
      "Epoch 70, Loss: 0.6600\n",
      "Epoch 80, Loss: 0.6736\n",
      "Epoch 90, Loss: 0.6189\n",
      "Epoch 100, Loss: 0.5884\n",
      "Epoch 110, Loss: 0.6606\n",
      "Epoch 120, Loss: 0.5640\n",
      "Epoch 130, Loss: 0.5651\n",
      "Epoch 140, Loss: 0.5342\n",
      "Epoch 150, Loss: 0.5563\n",
      "Epoch 160, Loss: 0.5256\n",
      "Epoch 170, Loss: 0.5370\n",
      "Epoch 180, Loss: 0.5281\n",
      "Epoch 190, Loss: 0.5351\n",
      "Epoch 200, Loss: 0.5040\n",
      "Epoch 210, Loss: 0.5469\n",
      "Epoch 220, Loss: 0.5462\n",
      "Epoch 230, Loss: 0.5054\n",
      "Epoch 240, Loss: 0.4909\n",
      "Epoch 250, Loss: 0.4669\n",
      "Epoch 260, Loss: 0.5071\n",
      "Epoch 270, Loss: 0.4848\n",
      "Epoch 280, Loss: 0.4628\n",
      "Epoch 290, Loss: 0.4530\n",
      "Epoch 300, Loss: 0.4627\n",
      "Epoch 310, Loss: 0.4746\n",
      "Epoch 320, Loss: 0.4594\n",
      "Epoch 330, Loss: 0.4359\n",
      "Epoch 340, Loss: 0.4545\n",
      "Epoch 350, Loss: 0.4546\n",
      "Epoch 360, Loss: 0.4380\n",
      "Epoch 370, Loss: 0.4422\n",
      "Epoch 380, Loss: 0.4524\n",
      "Epoch 390, Loss: 0.4175\n",
      "Epoch 400, Loss: 0.4438\n",
      "Epoch 410, Loss: 0.4384\n",
      "Epoch 420, Loss: 0.4250\n",
      "Epoch 430, Loss: 0.4339\n",
      "Epoch 440, Loss: 0.4369\n",
      "Epoch 450, Loss: 0.4086\n",
      "Epoch 460, Loss: 0.4543\n",
      "Epoch 470, Loss: 0.4169\n",
      "Epoch 480, Loss: 0.4154\n",
      "Epoch 490, Loss: 0.4134\n",
      "Epoch 500, Loss: 0.4044\n",
      "Epoch 510, Loss: 0.4020\n",
      "Epoch 520, Loss: 0.4331\n",
      "Epoch 530, Loss: 0.4010\n",
      "Epoch 540, Loss: 0.3949\n",
      "Epoch 550, Loss: 0.4015\n",
      "Epoch 560, Loss: 0.4116\n",
      "Epoch 570, Loss: 0.3871\n",
      "Epoch 580, Loss: 0.3979\n",
      "Epoch 590, Loss: 0.4267\n",
      "Epoch 600, Loss: 0.3885\n",
      "Epoch 610, Loss: 0.4029\n",
      "Epoch 620, Loss: 0.3975\n",
      "Epoch 630, Loss: 0.3968\n",
      "Epoch 640, Loss: 0.3906\n",
      "Epoch 650, Loss: 0.3736\n",
      "Epoch 660, Loss: 0.3931\n",
      "Epoch 670, Loss: 0.4070\n",
      "Epoch 680, Loss: 0.3741\n",
      "Epoch 690, Loss: 0.3922\n",
      "Epoch 700, Loss: 0.4032\n",
      "Epoch 710, Loss: 0.3867\n",
      "Epoch 720, Loss: 0.3771\n",
      "Epoch 730, Loss: 0.3840\n",
      "Epoch 740, Loss: 0.3715\n",
      "Epoch 750, Loss: 0.3818\n",
      "Epoch 760, Loss: 0.3718\n",
      "Epoch 770, Loss: 0.3836\n",
      "Epoch 780, Loss: 0.3710\n",
      "Epoch 790, Loss: 0.3688\n",
      "Epoch 800, Loss: 0.3736\n",
      "Epoch 810, Loss: 0.3665\n",
      "Epoch 820, Loss: 0.3662\n",
      "Epoch 830, Loss: 0.3587\n",
      "Epoch 840, Loss: 0.3852\n",
      "Epoch 850, Loss: 0.3615\n",
      "Epoch 860, Loss: 0.3556\n",
      "Epoch 870, Loss: 0.3549\n",
      "Epoch 880, Loss: 0.3828\n",
      "Epoch 890, Loss: 0.3695\n",
      "Epoch 900, Loss: 0.3579\n",
      "Epoch 910, Loss: 0.3602\n",
      "Epoch 920, Loss: 0.3476\n",
      "Epoch 930, Loss: 0.3710\n",
      "Epoch 940, Loss: 0.3722\n",
      "Epoch 950, Loss: 0.3472\n",
      "Epoch 960, Loss: 0.3496\n",
      "Epoch 970, Loss: 0.3607\n",
      "Epoch 980, Loss: 0.3618\n",
      "Epoch 990, Loss: 0.3451\n",
      "Epoch 1000, Loss: 0.3674\n",
      "Epoch 1010, Loss: 0.3489\n",
      "Epoch 1020, Loss: 0.3570\n",
      "Epoch 1030, Loss: 0.3417\n",
      "Epoch 1040, Loss: 0.3540\n",
      "Epoch 1050, Loss: 0.3590\n",
      "Epoch 1060, Loss: 0.3428\n",
      "Epoch 1070, Loss: 0.3444\n",
      "Epoch 1080, Loss: 0.3430\n",
      "Epoch 1090, Loss: 0.3647\n",
      "Epoch 1100, Loss: 0.3607\n",
      "Epoch 1110, Loss: 0.3339\n",
      "Epoch 1120, Loss: 0.3508\n",
      "Epoch 1130, Loss: 0.3483\n",
      "Epoch 1140, Loss: 0.3390\n",
      "Epoch 1150, Loss: 0.3344\n",
      "Epoch 1160, Loss: 0.3652\n",
      "Epoch 1170, Loss: 0.3376\n",
      "Epoch 1180, Loss: 0.3339\n",
      "Epoch 1190, Loss: 0.3503\n",
      "Epoch 1200, Loss: 0.3281\n",
      "Epoch 1210, Loss: 0.3357\n",
      "Epoch 1220, Loss: 0.3368\n",
      "Epoch 1230, Loss: 0.3447\n",
      "Epoch 1240, Loss: 0.3436\n",
      "Epoch 1250, Loss: 0.3482\n",
      "Epoch 1260, Loss: 0.3224\n",
      "Epoch 1270, Loss: 0.3405\n",
      "Epoch 1280, Loss: 0.3373\n",
      "Epoch 1290, Loss: 0.3267\n",
      "Epoch 1300, Loss: 0.3249\n",
      "Epoch 1310, Loss: 0.3488\n",
      "Epoch 1320, Loss: 0.3367\n",
      "Epoch 1330, Loss: 0.3299\n",
      "Epoch 1340, Loss: 0.3578\n",
      "Epoch 1350, Loss: 0.3450\n",
      "Epoch 1360, Loss: 0.3281\n",
      "Epoch 1370, Loss: 0.3358\n",
      "Epoch 1380, Loss: 0.3208\n",
      "Epoch 1390, Loss: 0.3205\n",
      "Epoch 1400, Loss: 0.3190\n",
      "Epoch 1410, Loss: 0.3177\n",
      "Epoch 1420, Loss: 0.3337\n",
      "Epoch 1430, Loss: 0.3291\n",
      "Epoch 1440, Loss: 0.3189\n",
      "Epoch 1450, Loss: 0.3184\n",
      "Epoch 1460, Loss: 0.3287\n",
      "Epoch 1470, Loss: 0.3244\n",
      "Epoch 1480, Loss: 0.3261\n",
      "Epoch 1490, Loss: 0.3139\n",
      "Epoch 1500, Loss: 0.3277\n",
      "Epoch 1510, Loss: 0.3154\n",
      "Epoch 1520, Loss: 0.3230\n",
      "Epoch 1530, Loss: 0.3386\n",
      "Epoch 1540, Loss: 0.3109\n",
      "Epoch 1550, Loss: 0.3129\n",
      "Epoch 1560, Loss: 0.3251\n",
      "Epoch 1570, Loss: 0.3265\n",
      "Epoch 1580, Loss: 0.3113\n",
      "Epoch 1590, Loss: 0.3129\n",
      "Epoch 1600, Loss: 0.3161\n",
      "Epoch 1610, Loss: 0.3461\n",
      "Epoch 1620, Loss: 0.3065\n",
      "Epoch 1630, Loss: 0.3137\n",
      "Epoch 1640, Loss: 0.3138\n",
      "Epoch 1650, Loss: 0.3230\n",
      "Epoch 1660, Loss: 0.3186\n",
      "Epoch 1670, Loss: 0.3151\n",
      "Epoch 1680, Loss: 0.3198\n",
      "Epoch 1690, Loss: 0.3204\n",
      "Epoch 1700, Loss: 0.3062\n",
      "Epoch 1710, Loss: 0.3101\n",
      "Epoch 1720, Loss: 0.3125\n",
      "Epoch 1730, Loss: 0.3080\n",
      "Epoch 1740, Loss: 0.3047\n",
      "Epoch 1750, Loss: 0.3024\n",
      "Epoch 1760, Loss: 0.3164\n",
      "Epoch 1770, Loss: 0.3100\n",
      "Epoch 1780, Loss: 0.3014\n",
      "Epoch 1790, Loss: 0.3080\n",
      "Epoch 1800, Loss: 0.3189\n",
      "Epoch 1810, Loss: 0.3007\n",
      "Epoch 1820, Loss: 0.3134\n",
      "Epoch 1830, Loss: 0.3066\n",
      "Epoch 1840, Loss: 0.3266\n",
      "Epoch 1850, Loss: 0.3133\n",
      "Epoch 1860, Loss: 0.3038\n",
      "Epoch 1870, Loss: 0.3030\n",
      "Epoch 1880, Loss: 0.2990\n",
      "Epoch 1890, Loss: 0.2991\n",
      "Epoch 1900, Loss: 0.2975\n",
      "Epoch 1910, Loss: 0.2951\n",
      "Epoch 1920, Loss: 0.2972\n",
      "Epoch 1930, Loss: 0.3039\n",
      "Epoch 1940, Loss: 0.2930\n",
      "Epoch 1950, Loss: 0.3132\n",
      "Epoch 1960, Loss: 0.3029\n",
      "Epoch 1970, Loss: 0.3268\n",
      "Epoch 1980, Loss: 0.2914\n",
      "Epoch 1990, Loss: 0.3151\n",
      "Epoch 2000, Loss: 0.2964\n",
      "Epoch 2010, Loss: 0.2994\n",
      "Epoch 2020, Loss: 0.3022\n",
      "Epoch 2030, Loss: 0.2985\n",
      "Epoch 2040, Loss: 0.2989\n",
      "Epoch 2050, Loss: 0.2999\n",
      "Epoch 2060, Loss: 0.2981\n",
      "Epoch 2070, Loss: 0.3125\n",
      "Epoch 2080, Loss: 0.2908\n",
      "Epoch 2090, Loss: 0.3002\n",
      "Epoch 2100, Loss: 0.2916\n",
      "Epoch 2110, Loss: 0.3062\n",
      "Epoch 2120, Loss: 0.2868\n",
      "Epoch 2130, Loss: 0.2875\n",
      "Epoch 2140, Loss: 0.2901\n",
      "Epoch 2150, Loss: 0.3005\n",
      "Epoch 2160, Loss: 0.2881\n",
      "Epoch 2170, Loss: 0.2938\n",
      "Epoch 2180, Loss: 0.2878\n",
      "Epoch 2190, Loss: 0.2876\n",
      "Epoch 2200, Loss: 0.2909\n",
      "Epoch 2210, Loss: 0.3167\n",
      "Epoch 2220, Loss: 0.2890\n",
      "Epoch 2230, Loss: 0.2950\n",
      "Epoch 2240, Loss: 0.2831\n",
      "Epoch 2250, Loss: 0.2813\n",
      "Epoch 2260, Loss: 0.2928\n",
      "Epoch 2270, Loss: 0.3120\n",
      "Epoch 2280, Loss: 0.2893\n",
      "Epoch 2290, Loss: 0.2902\n",
      "Epoch 2300, Loss: 0.2906\n",
      "Epoch 2310, Loss: 0.2837\n",
      "Epoch 2320, Loss: 0.2775\n",
      "Epoch 2330, Loss: 0.2834\n",
      "Epoch 2340, Loss: 0.2953\n",
      "Epoch 2350, Loss: 0.2820\n",
      "Epoch 2360, Loss: 0.2817\n",
      "Epoch 2370, Loss: 0.2860\n",
      "Epoch 2380, Loss: 0.2998\n",
      "Epoch 2390, Loss: 0.2926\n",
      "Epoch 2400, Loss: 0.2828\n",
      "Epoch 2410, Loss: 0.2877\n",
      "Epoch 2420, Loss: 0.2884\n",
      "Epoch 2430, Loss: 0.2934\n",
      "Epoch 2440, Loss: 0.2826\n",
      "Epoch 2450, Loss: 0.2793\n",
      "Epoch 2460, Loss: 0.2833\n",
      "Epoch 2470, Loss: 0.2866\n",
      "Epoch 2480, Loss: 0.2892\n",
      "Epoch 2490, Loss: 0.2919\n",
      "Epoch 2500, Loss: 0.2796\n",
      "Epoch 2510, Loss: 0.2804\n",
      "Epoch 2520, Loss: 0.2763\n",
      "Epoch 2530, Loss: 0.2751\n",
      "Epoch 2540, Loss: 0.2891\n",
      "Epoch 2550, Loss: 0.2852\n",
      "Epoch 2560, Loss: 0.2796\n",
      "Epoch 2570, Loss: 0.2827\n",
      "Epoch 2580, Loss: 0.2754\n",
      "Epoch 2590, Loss: 0.2726\n",
      "Epoch 2600, Loss: 0.2903\n",
      "Epoch 2610, Loss: 0.2795\n",
      "Epoch 2620, Loss: 0.2781\n",
      "Epoch 2630, Loss: 0.2734\n",
      "Epoch 2640, Loss: 0.2821\n",
      "Epoch 2650, Loss: 0.2751\n",
      "Epoch 2660, Loss: 0.2873\n",
      "Epoch 2670, Loss: 0.2723\n",
      "Epoch 2680, Loss: 0.2749\n",
      "Epoch 2690, Loss: 0.2763\n",
      "Epoch 2700, Loss: 0.2831\n",
      "Epoch 2710, Loss: 0.2735\n",
      "Epoch 2720, Loss: 0.2676\n",
      "Epoch 2730, Loss: 0.2693\n",
      "Epoch 2740, Loss: 0.2747\n",
      "Epoch 2750, Loss: 0.2707\n",
      "Epoch 2760, Loss: 0.2862\n",
      "Epoch 2770, Loss: 0.2767\n",
      "Epoch 2780, Loss: 0.2670\n",
      "Epoch 2790, Loss: 0.2688\n",
      "Epoch 2800, Loss: 0.2785\n",
      "Epoch 2810, Loss: 0.2712\n",
      "Epoch 2820, Loss: 0.2828\n",
      "Epoch 2830, Loss: 0.2838\n",
      "Epoch 2840, Loss: 0.2743\n",
      "Epoch 2850, Loss: 0.2703\n",
      "Epoch 2860, Loss: 0.2733\n",
      "Epoch 2870, Loss: 0.2716\n",
      "Epoch 2880, Loss: 0.2685\n",
      "Epoch 2890, Loss: 0.2683\n",
      "Epoch 2900, Loss: 0.2841\n",
      "Epoch 2910, Loss: 0.2713\n",
      "Epoch 2920, Loss: 0.2719\n",
      "Epoch 2930, Loss: 0.2668\n",
      "Epoch 2940, Loss: 0.2625\n",
      "Epoch 2950, Loss: 0.2647\n",
      "Epoch 2960, Loss: 0.2712\n",
      "Epoch 2970, Loss: 0.2759\n",
      "Epoch 2980, Loss: 0.2673\n",
      "Epoch 2990, Loss: 0.2742\n"
     ]
    }
   ],
   "source": [
    "# Train without regularization\n",
    "nn_no_reg = NeuralNetwork(input_size, hidden_size, output_size, learning_rate)\n",
    "nn_no_reg.train(x_train, y_train_encoded, epochs)\n",
    "predictions_no_reg = nn_no_reg.predict(x_test)\n",
    "accuracy_no_reg = np.mean(predictions_no_reg == y_test)\n",
    "print(f\"Test Accuracy (No Regularization): {accuracy_no_reg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87e81c6-0703-4799-bcaa-ae57cc4edf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "predictions = nn.predict(x_test)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bad74ef-bbd4-47bb-b289-3ae3f211e29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "def plot_predictions(X, y_true, y_pred, num_images=10):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(X[i].reshape(28, 28), cmap='gray')\n",
    "        plt.title(f'True: {y_true[i]}\\nPred: {y_pred[i]}')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize predictions on test set\n",
    "plot_predictions(x_test, y_test, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
